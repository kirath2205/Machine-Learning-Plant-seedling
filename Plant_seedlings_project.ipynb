{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant_seedlings_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP92LLPjeGR5K8UvOzDH8xZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Machine-Learning-Plant-seedling/blob/main/Plant_seedlings_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jLKQK3N4V9-"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRsevFzr5OUZ"
      },
      "source": [
        "def define_generators():\n",
        "  height = 250\n",
        "  width = 250\n",
        "  batch_size = 64\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=360,\n",
        "        width_shift_range=0.3,\n",
        "        height_shift_range=0.3,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.5,\n",
        "        vertical_flip=True,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2, # change to use validation instead of training on entire training set\n",
        "    )\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "        directory='Documents/train',\n",
        "        target_size=(width, height),\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode=\"categorical\",\n",
        "        subset='training',\n",
        "    )\n",
        "\n",
        "  validation_generator = train_datagen.flow_from_directory(\n",
        "        directory='Documents/train',\n",
        "        target_size=(width, height),\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode=\"categorical\",\n",
        "        subset='validation',\n",
        "    )\n",
        "\n",
        "  test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  test_generator = test_datagen.flow_from_directory(\n",
        "        directory='Documents/',\n",
        "        classes=['test'],\n",
        "        target_size=(width, height),\n",
        "        batch_size=1,\n",
        "        color_mode='rgb',\n",
        "        shuffle=False,\n",
        "        class_mode='categorical')\n",
        "\n",
        "  return train_generator, validation_generator, test_generator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAUpGw6D6ip9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def define_model_vgg16_improvement(image_shape,total_classes):\n",
        "  print(image_shape[2])\n",
        "\n",
        "  model = Sequential()\n",
        "  weight_decay = 0.0005\n",
        "  learning_rate = 0.1\n",
        "  lr_decay = 1e-6\n",
        "  lr_drop = 20\n",
        "  \n",
        "  model.add(Conv2D(64, (3, 3), padding='same',input_shape=image_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  #model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(total_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  sgd = keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNZV6W7O9s2O"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "def train_model_vgg16(model , train_generator,validation_generator,epochs=10,batch_size=128):\n",
        "  \n",
        "  callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model_vgg16_plant_seedlings', monitor='val_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.9, patience = 3, min_lr = 0.000001, verbose = 1 ),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor = 'val_loss' , patience = 40)\n",
        "  ]\n",
        "  history=model.fit(train_generator, epochs = epochs, batch_size = batch_size,callbacks = callbacks, verbose = 1,validation_data = validation_generator)\n",
        "  return history"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An9BB54h9xY2",
        "outputId": "44f95003-c035-495f-80a0-e59bae68587a"
      },
      "source": [
        "\n",
        "train_generator, validation_generator, test_generator  = define_generators()\n",
        "improved_model = define_model_vgg16_improvement((250, 250 , 3),12)\n",
        "history = train_model_vgg16(improved_model , train_generator , validation_generator , epochs=250 , batch_size = 64)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3803 images belonging to 12 classes.\n",
            "Found 947 images belonging to 12 classes.\n",
            "Found 794 images belonging to 1 classes.\n",
            "3\n",
            "Epoch 1/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 100.3854 - accuracy: 0.1386\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.10876, saving model to best_model_vgg16_plant_seedlings\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 76s 1s/step - loss: 100.3854 - accuracy: 0.1386 - val_loss: 132497.5156 - val_accuracy: 0.1088 - lr: 0.1000\n",
            "Epoch 2/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 116.1353 - accuracy: 0.1917\n",
            "Epoch 00002: val_accuracy did not improve from 0.10876\n",
            "60/60 [==============================] - 67s 1s/step - loss: 116.1353 - accuracy: 0.1917 - val_loss: 168.6592 - val_accuracy: 0.0834 - lr: 0.1000\n",
            "Epoch 3/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 103.0173 - accuracy: 0.2861\n",
            "Epoch 00003: val_accuracy improved from 0.10876 to 0.11088, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 103.0173 - accuracy: 0.2861 - val_loss: 98.2720 - val_accuracy: 0.1109 - lr: 0.1000\n",
            "Epoch 4/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 91.4771 - accuracy: 0.3595\n",
            "Epoch 00004: val_accuracy improved from 0.11088 to 0.17846, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 91.4771 - accuracy: 0.3595 - val_loss: 87.1112 - val_accuracy: 0.1785 - lr: 0.1000\n",
            "Epoch 5/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 81.5910 - accuracy: 0.4191\n",
            "Epoch 00005: val_accuracy improved from 0.17846 to 0.20169, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 81.5910 - accuracy: 0.4191 - val_loss: 77.7687 - val_accuracy: 0.2017 - lr: 0.1000\n",
            "Epoch 6/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 72.3876 - accuracy: 0.4744\n",
            "Epoch 00006: val_accuracy did not improve from 0.20169\n",
            "60/60 [==============================] - 67s 1s/step - loss: 72.3876 - accuracy: 0.4744 - val_loss: 69.5477 - val_accuracy: 0.1668 - lr: 0.1000\n",
            "Epoch 7/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 64.3356 - accuracy: 0.4978\n",
            "Epoch 00007: val_accuracy improved from 0.20169 to 0.28194, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 64.3356 - accuracy: 0.4978 - val_loss: 61.5548 - val_accuracy: 0.2819 - lr: 0.1000\n",
            "Epoch 8/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 57.0839 - accuracy: 0.5314\n",
            "Epoch 00008: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 57.0839 - accuracy: 0.5314 - val_loss: 55.3707 - val_accuracy: 0.1911 - lr: 0.1000\n",
            "Epoch 9/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 50.6498 - accuracy: 0.5688\n",
            "Epoch 00009: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 50.6498 - accuracy: 0.5688 - val_loss: 49.5062 - val_accuracy: 0.2703 - lr: 0.1000\n",
            "Epoch 10/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 44.9348 - accuracy: 0.6027\n",
            "Epoch 00010: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 44.9348 - accuracy: 0.6027 - val_loss: 44.3930 - val_accuracy: 0.2270 - lr: 0.1000\n",
            "Epoch 11/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 39.9407 - accuracy: 0.6195\n",
            "Epoch 00011: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 68s 1s/step - loss: 39.9407 - accuracy: 0.6195 - val_loss: 39.7161 - val_accuracy: 0.2482 - lr: 0.1000\n",
            "Epoch 12/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 35.5339 - accuracy: 0.6334\n",
            "Epoch 00012: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 35.5339 - accuracy: 0.6334 - val_loss: 36.7925 - val_accuracy: 0.2334 - lr: 0.1000\n",
            "Epoch 13/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 31.6380 - accuracy: 0.6371\n",
            "Epoch 00013: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 68s 1s/step - loss: 31.6380 - accuracy: 0.6371 - val_loss: 33.3190 - val_accuracy: 0.2122 - lr: 0.1000\n",
            "Epoch 14/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 28.1154 - accuracy: 0.6584\n",
            "Epoch 00014: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 28.1154 - accuracy: 0.6584 - val_loss: 30.0883 - val_accuracy: 0.1975 - lr: 0.1000\n",
            "Epoch 15/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 24.9985 - accuracy: 0.6876\n",
            "Epoch 00015: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 24.9985 - accuracy: 0.6876 - val_loss: 27.6491 - val_accuracy: 0.1742 - lr: 0.1000\n",
            "Epoch 16/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 22.2980 - accuracy: 0.6842\n",
            "Epoch 00016: val_accuracy did not improve from 0.28194\n",
            "60/60 [==============================] - 67s 1s/step - loss: 22.2980 - accuracy: 0.6842 - val_loss: 23.7974 - val_accuracy: 0.2777 - lr: 0.1000\n",
            "Epoch 17/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 19.8583 - accuracy: 0.7155\n",
            "Epoch 00017: val_accuracy improved from 0.28194 to 0.46040, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 19.8583 - accuracy: 0.7155 - val_loss: 19.5487 - val_accuracy: 0.4604 - lr: 0.1000\n",
            "Epoch 18/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 17.6896 - accuracy: 0.7152\n",
            "Epoch 00018: val_accuracy did not improve from 0.46040\n",
            "60/60 [==============================] - 67s 1s/step - loss: 17.6896 - accuracy: 0.7152 - val_loss: 18.0578 - val_accuracy: 0.3939 - lr: 0.1000\n",
            "Epoch 19/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 15.8183 - accuracy: 0.7255\n",
            "Epoch 00019: val_accuracy did not improve from 0.46040\n",
            "60/60 [==============================] - 67s 1s/step - loss: 15.8183 - accuracy: 0.7255 - val_loss: 17.0185 - val_accuracy: 0.2904 - lr: 0.1000\n",
            "Epoch 20/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 14.1687 - accuracy: 0.7202\n",
            "Epoch 00020: val_accuracy improved from 0.46040 to 0.49525, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 14.1687 - accuracy: 0.7202 - val_loss: 14.1526 - val_accuracy: 0.4952 - lr: 0.1000\n",
            "Epoch 21/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 12.6909 - accuracy: 0.7363\n",
            "Epoch 00021: val_accuracy did not improve from 0.49525\n",
            "60/60 [==============================] - 68s 1s/step - loss: 12.6909 - accuracy: 0.7363 - val_loss: 16.1435 - val_accuracy: 0.1975 - lr: 0.1000\n",
            "Epoch 22/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 11.4558 - accuracy: 0.7347\n",
            "Epoch 00022: val_accuracy improved from 0.49525 to 0.51426, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 11.4558 - accuracy: 0.7347 - val_loss: 11.6268 - val_accuracy: 0.5143 - lr: 0.1000\n",
            "Epoch 23/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 10.3290 - accuracy: 0.7441\n",
            "Epoch 00023: val_accuracy did not improve from 0.51426\n",
            "60/60 [==============================] - 67s 1s/step - loss: 10.3290 - accuracy: 0.7441 - val_loss: 10.9472 - val_accuracy: 0.4805 - lr: 0.1000\n",
            "Epoch 24/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 9.3317 - accuracy: 0.7405\n",
            "Epoch 00024: val_accuracy improved from 0.51426 to 0.51637, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 9.3317 - accuracy: 0.7405 - val_loss: 9.9486 - val_accuracy: 0.5164 - lr: 0.1000\n",
            "Epoch 25/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 8.4263 - accuracy: 0.7305\n",
            "Epoch 00025: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 8.4263 - accuracy: 0.7305 - val_loss: 10.0091 - val_accuracy: 0.3442 - lr: 0.1000\n",
            "Epoch 26/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 7.5557 - accuracy: 0.7573\n",
            "Epoch 00026: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 7.5557 - accuracy: 0.7573 - val_loss: 8.4326 - val_accuracy: 0.4889 - lr: 0.1000\n",
            "Epoch 27/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 8.0342 - accuracy: 0.5496\n",
            "Epoch 00027: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 8.0342 - accuracy: 0.5496 - val_loss: 147.2798 - val_accuracy: 0.0465 - lr: 0.1000\n",
            "Epoch 28/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 8.2648 - accuracy: 0.5393\n",
            "Epoch 00028: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 8.2648 - accuracy: 0.5393 - val_loss: 10.9921 - val_accuracy: 0.2693 - lr: 0.1000\n",
            "Epoch 29/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 7.3970 - accuracy: 0.6692\n",
            "Epoch 00029: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 7.3970 - accuracy: 0.6692 - val_loss: 7.9576 - val_accuracy: 0.4541 - lr: 0.1000\n",
            "Epoch 30/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 6.6311 - accuracy: 0.7010\n",
            "Epoch 00030: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 6.6311 - accuracy: 0.7010 - val_loss: 11.5110 - val_accuracy: 0.1869 - lr: 0.1000\n",
            "Epoch 31/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 5.9563 - accuracy: 0.7278\n",
            "Epoch 00031: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 5.9563 - accuracy: 0.7278 - val_loss: 7.1214 - val_accuracy: 0.3633 - lr: 0.1000\n",
            "Epoch 32/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 5.3861 - accuracy: 0.7449\n",
            "Epoch 00032: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 5.3861 - accuracy: 0.7449 - val_loss: 6.9079 - val_accuracy: 0.3696 - lr: 0.1000\n",
            "Epoch 33/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 4.8728 - accuracy: 0.7576\n",
            "Epoch 00033: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 4.8728 - accuracy: 0.7576 - val_loss: 6.1095 - val_accuracy: 0.4065 - lr: 0.1000\n",
            "Epoch 34/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 4.4685 - accuracy: 0.7615\n",
            "Epoch 00034: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 4.4685 - accuracy: 0.7615 - val_loss: 6.7287 - val_accuracy: 0.3326 - lr: 0.1000\n",
            "Epoch 35/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 4.1278 - accuracy: 0.7578\n",
            "Epoch 00035: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 4.1278 - accuracy: 0.7578 - val_loss: 5.1059 - val_accuracy: 0.4752 - lr: 0.1000\n",
            "Epoch 36/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.8117 - accuracy: 0.7578\n",
            "Epoch 00036: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 3.8117 - accuracy: 0.7578 - val_loss: 6.5757 - val_accuracy: 0.3200 - lr: 0.1000\n",
            "Epoch 37/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.5396 - accuracy: 0.7639\n",
            "Epoch 00037: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 68s 1s/step - loss: 3.5396 - accuracy: 0.7639 - val_loss: 4.8288 - val_accuracy: 0.5143 - lr: 0.1000\n",
            "Epoch 38/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.3223 - accuracy: 0.7541\n",
            "Epoch 00038: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 3.3223 - accuracy: 0.7541 - val_loss: 4.2949 - val_accuracy: 0.4889 - lr: 0.1000\n",
            "Epoch 39/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.0503 - accuracy: 0.7728\n",
            "Epoch 00039: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 3.0503 - accuracy: 0.7728 - val_loss: 5.4810 - val_accuracy: 0.4065 - lr: 0.1000\n",
            "Epoch 40/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.8200 - accuracy: 0.7781\n",
            "Epoch 00040: val_accuracy did not improve from 0.51637\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.8200 - accuracy: 0.7781 - val_loss: 4.4966 - val_accuracy: 0.4604 - lr: 0.1000\n",
            "Epoch 41/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.6697 - accuracy: 0.7697\n",
            "Epoch 00041: val_accuracy improved from 0.51637 to 0.59451, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 2.6697 - accuracy: 0.7697 - val_loss: 3.1920 - val_accuracy: 0.5945 - lr: 0.1000\n",
            "Epoch 42/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4649 - accuracy: 0.7760\n",
            "Epoch 00042: val_accuracy improved from 0.59451 to 0.61457, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 2.4649 - accuracy: 0.7760 - val_loss: 3.2080 - val_accuracy: 0.6146 - lr: 0.1000\n",
            "Epoch 43/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.3551 - accuracy: 0.7778\n",
            "Epoch 00043: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.3551 - accuracy: 0.7778 - val_loss: 8.6237 - val_accuracy: 0.2080 - lr: 0.1000\n",
            "Epoch 44/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.2249 - accuracy: 0.7747\n",
            "Epoch 00044: val_accuracy did not improve from 0.61457\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.09000000134110452.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 2.2249 - accuracy: 0.7747 - val_loss: 8.4084 - val_accuracy: 0.1996 - lr: 0.1000\n",
            "Epoch 45/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.0881 - accuracy: 0.7896\n",
            "Epoch 00045: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 68s 1s/step - loss: 2.0881 - accuracy: 0.7896 - val_loss: 5.3784 - val_accuracy: 0.3210 - lr: 0.0900\n",
            "Epoch 46/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.0082 - accuracy: 0.7941\n",
            "Epoch 00046: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 68s 1s/step - loss: 2.0082 - accuracy: 0.7941 - val_loss: 5.1034 - val_accuracy: 0.3696 - lr: 0.0900\n",
            "Epoch 47/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.1699 - accuracy: 0.7744\n",
            "Epoch 00047: val_accuracy did not improve from 0.61457\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.08100000321865082.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.1699 - accuracy: 0.7744 - val_loss: 4.1538 - val_accuracy: 0.4013 - lr: 0.0900\n",
            "Epoch 48/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.4468 - accuracy: 0.7602\n",
            "Epoch 00048: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.4468 - accuracy: 0.7602 - val_loss: 2.9234 - val_accuracy: 0.5956 - lr: 0.0810\n",
            "Epoch 49/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.2217 - accuracy: 0.7981\n",
            "Epoch 00049: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.2217 - accuracy: 0.7981 - val_loss: 5.4882 - val_accuracy: 0.3411 - lr: 0.0810\n",
            "Epoch 50/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.1281 - accuracy: 0.7910\n",
            "Epoch 00050: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.1281 - accuracy: 0.7910 - val_loss: 4.4897 - val_accuracy: 0.3041 - lr: 0.0810\n",
            "Epoch 51/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.0158 - accuracy: 0.7894\n",
            "Epoch 00051: val_accuracy did not improve from 0.61457\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.07290000021457672.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 2.0158 - accuracy: 0.7894 - val_loss: 3.6414 - val_accuracy: 0.4530 - lr: 0.0810\n",
            "Epoch 52/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.8871 - accuracy: 0.8099\n",
            "Epoch 00052: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.8871 - accuracy: 0.8099 - val_loss: 6.1335 - val_accuracy: 0.2777 - lr: 0.0729\n",
            "Epoch 53/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.7337 - accuracy: 0.8341\n",
            "Epoch 00053: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.7337 - accuracy: 0.8341 - val_loss: 2.8218 - val_accuracy: 0.5966 - lr: 0.0729\n",
            "Epoch 54/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.6998 - accuracy: 0.8138\n",
            "Epoch 00054: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.6998 - accuracy: 0.8138 - val_loss: 3.6445 - val_accuracy: 0.4361 - lr: 0.0729\n",
            "Epoch 55/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.6424 - accuracy: 0.8175\n",
            "Epoch 00055: val_accuracy did not improve from 0.61457\n",
            "60/60 [==============================] - 66s 1s/step - loss: 1.6424 - accuracy: 0.8175 - val_loss: 4.7041 - val_accuracy: 0.2492 - lr: 0.0729\n",
            "Epoch 56/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.5510 - accuracy: 0.8270\n",
            "Epoch 00056: val_accuracy improved from 0.61457 to 0.63147, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 1.5510 - accuracy: 0.8270 - val_loss: 2.0818 - val_accuracy: 0.6315 - lr: 0.0729\n",
            "Epoch 57/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.5435 - accuracy: 0.8199\n",
            "Epoch 00057: val_accuracy did not improve from 0.63147\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.5435 - accuracy: 0.8199 - val_loss: 3.7595 - val_accuracy: 0.4192 - lr: 0.0729\n",
            "Epoch 58/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.5180 - accuracy: 0.8199\n",
            "Epoch 00058: val_accuracy did not improve from 0.63147\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.5180 - accuracy: 0.8199 - val_loss: 6.0629 - val_accuracy: 0.3421 - lr: 0.0729\n",
            "Epoch 59/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.5256 - accuracy: 0.8115\n",
            "Epoch 00059: val_accuracy did not improve from 0.63147\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.06560999751091004.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.5256 - accuracy: 0.8115 - val_loss: 5.6345 - val_accuracy: 0.3157 - lr: 0.0729\n",
            "Epoch 60/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.4594 - accuracy: 0.8178\n",
            "Epoch 00060: val_accuracy did not improve from 0.63147\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.4594 - accuracy: 0.8178 - val_loss: 5.3086 - val_accuracy: 0.3601 - lr: 0.0656\n",
            "Epoch 61/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.4004 - accuracy: 0.8343\n",
            "Epoch 00061: val_accuracy did not improve from 0.63147\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.4004 - accuracy: 0.8343 - val_loss: 3.2659 - val_accuracy: 0.4593 - lr: 0.0656\n",
            "Epoch 62/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.3178 - accuracy: 0.8496\n",
            "Epoch 00062: val_accuracy did not improve from 0.63147\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.05904899910092354.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.3178 - accuracy: 0.8496 - val_loss: 3.5859 - val_accuracy: 0.4319 - lr: 0.0656\n",
            "Epoch 63/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.2949 - accuracy: 0.8396\n",
            "Epoch 00063: val_accuracy improved from 0.63147 to 0.74868, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 1.2949 - accuracy: 0.8396 - val_loss: 1.5912 - val_accuracy: 0.7487 - lr: 0.0590\n",
            "Epoch 64/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.2638 - accuracy: 0.8364\n",
            "Epoch 00064: val_accuracy did not improve from 0.74868\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.2638 - accuracy: 0.8364 - val_loss: 6.0584 - val_accuracy: 0.2598 - lr: 0.0590\n",
            "Epoch 65/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.2674 - accuracy: 0.8388\n",
            "Epoch 00065: val_accuracy did not improve from 0.74868\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.2674 - accuracy: 0.8388 - val_loss: 4.3605 - val_accuracy: 0.3812 - lr: 0.0590\n",
            "Epoch 66/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.3086 - accuracy: 0.8270\n",
            "Epoch 00066: val_accuracy did not improve from 0.74868\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.053144099190831184.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.3086 - accuracy: 0.8270 - val_loss: 3.8605 - val_accuracy: 0.3949 - lr: 0.0590\n",
            "Epoch 67/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1979 - accuracy: 0.8493\n",
            "Epoch 00067: val_accuracy improved from 0.74868 to 0.76874, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 1.1979 - accuracy: 0.8493 - val_loss: 1.4578 - val_accuracy: 0.7687 - lr: 0.0531\n",
            "Epoch 68/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1923 - accuracy: 0.8467\n",
            "Epoch 00068: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.1923 - accuracy: 0.8467 - val_loss: 2.0300 - val_accuracy: 0.6030 - lr: 0.0531\n",
            "Epoch 69/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1316 - accuracy: 0.8591\n",
            "Epoch 00069: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.1316 - accuracy: 0.8591 - val_loss: 2.1667 - val_accuracy: 0.6177 - lr: 0.0531\n",
            "Epoch 70/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1281 - accuracy: 0.8493\n",
            "Epoch 00070: val_accuracy did not improve from 0.76874\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.04782968759536743.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.1281 - accuracy: 0.8493 - val_loss: 2.8830 - val_accuracy: 0.3749 - lr: 0.0531\n",
            "Epoch 71/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0578 - accuracy: 0.8688\n",
            "Epoch 00071: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.0578 - accuracy: 0.8688 - val_loss: 1.9909 - val_accuracy: 0.6251 - lr: 0.0478\n",
            "Epoch 72/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.8683\n",
            "Epoch 00072: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.0374 - accuracy: 0.8683 - val_loss: 1.9055 - val_accuracy: 0.6420 - lr: 0.0478\n",
            "Epoch 73/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0143 - accuracy: 0.8691\n",
            "Epoch 00073: val_accuracy did not improve from 0.76874\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.04304671883583069.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.0143 - accuracy: 0.8691 - val_loss: 4.1208 - val_accuracy: 0.3464 - lr: 0.0478\n",
            "Epoch 74/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0184 - accuracy: 0.8672\n",
            "Epoch 00074: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 1.0184 - accuracy: 0.8672 - val_loss: 1.5735 - val_accuracy: 0.6790 - lr: 0.0430\n",
            "Epoch 75/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.9799 - accuracy: 0.8719\n",
            "Epoch 00075: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.9799 - accuracy: 0.8719 - val_loss: 2.0319 - val_accuracy: 0.6346 - lr: 0.0430\n",
            "Epoch 76/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.9543 - accuracy: 0.8748\n",
            "Epoch 00076: val_accuracy did not improve from 0.76874\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.03874204829335213.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.9543 - accuracy: 0.8748 - val_loss: 3.2914 - val_accuracy: 0.4002 - lr: 0.0430\n",
            "Epoch 77/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.9378 - accuracy: 0.8725\n",
            "Epoch 00077: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.9378 - accuracy: 0.8725 - val_loss: 1.7919 - val_accuracy: 0.6653 - lr: 0.0387\n",
            "Epoch 78/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.8783\n",
            "Epoch 00078: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.9111 - accuracy: 0.8783 - val_loss: 2.2609 - val_accuracy: 0.6114 - lr: 0.0387\n",
            "Epoch 79/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8904 - accuracy: 0.8864\n",
            "Epoch 00079: val_accuracy did not improve from 0.76874\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.034867842122912406.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.8904 - accuracy: 0.8864 - val_loss: 1.9897 - val_accuracy: 0.5628 - lr: 0.0387\n",
            "Epoch 80/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8696 - accuracy: 0.8825\n",
            "Epoch 00080: val_accuracy did not improve from 0.76874\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.8696 - accuracy: 0.8825 - val_loss: 1.3402 - val_accuracy: 0.7603 - lr: 0.0349\n",
            "Epoch 81/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8540 - accuracy: 0.8867\n",
            "Epoch 00081: val_accuracy improved from 0.76874 to 0.81943, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.8540 - accuracy: 0.8867 - val_loss: 1.1287 - val_accuracy: 0.8194 - lr: 0.0349\n",
            "Epoch 82/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8475 - accuracy: 0.8859\n",
            "Epoch 00082: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.8475 - accuracy: 0.8859 - val_loss: 1.3680 - val_accuracy: 0.7265 - lr: 0.0349\n",
            "Epoch 83/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.8817\n",
            "Epoch 00083: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.8352 - accuracy: 0.8817 - val_loss: 1.1291 - val_accuracy: 0.8036 - lr: 0.0349\n",
            "Epoch 84/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8358 - accuracy: 0.8762\n",
            "Epoch 00084: val_accuracy did not improve from 0.81943\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.03138105757534504.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.8358 - accuracy: 0.8762 - val_loss: 2.3628 - val_accuracy: 0.4456 - lr: 0.0349\n",
            "Epoch 85/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.8861\n",
            "Epoch 00085: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.8273 - accuracy: 0.8861 - val_loss: 3.0811 - val_accuracy: 0.4604 - lr: 0.0314\n",
            "Epoch 86/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7907 - accuracy: 0.8935\n",
            "Epoch 00086: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7907 - accuracy: 0.8935 - val_loss: 1.4699 - val_accuracy: 0.6705 - lr: 0.0314\n",
            "Epoch 87/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7983 - accuracy: 0.8906\n",
            "Epoch 00087: val_accuracy did not improve from 0.81943\n",
            "\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.028242950141429902.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7983 - accuracy: 0.8906 - val_loss: 2.6533 - val_accuracy: 0.4974 - lr: 0.0314\n",
            "Epoch 88/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7427 - accuracy: 0.9035\n",
            "Epoch 00088: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7427 - accuracy: 0.9035 - val_loss: 1.2659 - val_accuracy: 0.7307 - lr: 0.0282\n",
            "Epoch 89/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7400 - accuracy: 0.9009\n",
            "Epoch 00089: val_accuracy did not improve from 0.81943\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7400 - accuracy: 0.9009 - val_loss: 1.4769 - val_accuracy: 0.6948 - lr: 0.0282\n",
            "Epoch 90/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7522 - accuracy: 0.8972\n",
            "Epoch 00090: val_accuracy did not improve from 0.81943\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.02541865445673466.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.7522 - accuracy: 0.8972 - val_loss: 1.2148 - val_accuracy: 0.7254 - lr: 0.0282\n",
            "Epoch 91/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7515 - accuracy: 0.8893\n",
            "Epoch 00091: val_accuracy improved from 0.81943 to 0.82682, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.7515 - accuracy: 0.8893 - val_loss: 0.9959 - val_accuracy: 0.8268 - lr: 0.0254\n",
            "Epoch 92/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7287 - accuracy: 0.8982\n",
            "Epoch 00092: val_accuracy did not improve from 0.82682\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7287 - accuracy: 0.8982 - val_loss: 1.5443 - val_accuracy: 0.7117 - lr: 0.0254\n",
            "Epoch 93/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7287 - accuracy: 0.8993\n",
            "Epoch 00093: val_accuracy did not improve from 0.82682\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.7287 - accuracy: 0.8993 - val_loss: 3.1947 - val_accuracy: 0.4065 - lr: 0.0254\n",
            "Epoch 94/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7074 - accuracy: 0.9006\n",
            "Epoch 00094: val_accuracy did not improve from 0.82682\n",
            "\n",
            "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.022876788675785065.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.7074 - accuracy: 0.9006 - val_loss: 1.0184 - val_accuracy: 0.8173 - lr: 0.0254\n",
            "Epoch 95/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.9024\n",
            "Epoch 00095: val_accuracy improved from 0.82682 to 0.89652, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.7022 - accuracy: 0.9024 - val_loss: 0.7541 - val_accuracy: 0.8965 - lr: 0.0229\n",
            "Epoch 96/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.9095\n",
            "Epoch 00096: val_accuracy did not improve from 0.89652\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6766 - accuracy: 0.9095 - val_loss: 1.0009 - val_accuracy: 0.8068 - lr: 0.0229\n",
            "Epoch 97/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.9014\n",
            "Epoch 00097: val_accuracy did not improve from 0.89652\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.6900 - accuracy: 0.9014 - val_loss: 1.1195 - val_accuracy: 0.7973 - lr: 0.0229\n",
            "Epoch 98/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.9153\n",
            "Epoch 00098: val_accuracy did not improve from 0.89652\n",
            "\n",
            "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.020589109137654306.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.6608 - accuracy: 0.9153 - val_loss: 2.8253 - val_accuracy: 0.5195 - lr: 0.0229\n",
            "Epoch 99/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.9140\n",
            "Epoch 00099: val_accuracy did not improve from 0.89652\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6355 - accuracy: 0.9140 - val_loss: 1.1896 - val_accuracy: 0.7582 - lr: 0.0206\n",
            "Epoch 100/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.9143\n",
            "Epoch 00100: val_accuracy did not improve from 0.89652\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.6481 - accuracy: 0.9143 - val_loss: 0.9955 - val_accuracy: 0.8163 - lr: 0.0206\n",
            "Epoch 101/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.9130\n",
            "Epoch 00101: val_accuracy did not improve from 0.89652\n",
            "\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.018530198559165.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.6403 - accuracy: 0.9130 - val_loss: 2.3601 - val_accuracy: 0.5312 - lr: 0.0206\n",
            "Epoch 102/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.9124\n",
            "Epoch 00102: val_accuracy did not improve from 0.89652\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6354 - accuracy: 0.9124 - val_loss: 1.1597 - val_accuracy: 0.7540 - lr: 0.0185\n",
            "Epoch 103/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.9185\n",
            "Epoch 00103: val_accuracy improved from 0.89652 to 0.91447, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.6122 - accuracy: 0.9185 - val_loss: 0.7059 - val_accuracy: 0.9145 - lr: 0.0185\n",
            "Epoch 104/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.9227\n",
            "Epoch 00104: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6077 - accuracy: 0.9227 - val_loss: 1.5254 - val_accuracy: 0.6642 - lr: 0.0185\n",
            "Epoch 105/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.9095\n",
            "Epoch 00105: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6301 - accuracy: 0.9095 - val_loss: 0.8460 - val_accuracy: 0.8617 - lr: 0.0185\n",
            "Epoch 106/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.9187\n",
            "Epoch 00106: val_accuracy did not improve from 0.91447\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.016677179373800755.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5990 - accuracy: 0.9187 - val_loss: 1.1475 - val_accuracy: 0.7719 - lr: 0.0185\n",
            "Epoch 107/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.9203\n",
            "Epoch 00107: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.6019 - accuracy: 0.9203 - val_loss: 0.8413 - val_accuracy: 0.8395 - lr: 0.0167\n",
            "Epoch 108/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.9287\n",
            "Epoch 00108: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5567 - accuracy: 0.9287 - val_loss: 0.8388 - val_accuracy: 0.8511 - lr: 0.0167\n",
            "Epoch 109/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.9237\n",
            "Epoch 00109: val_accuracy did not improve from 0.91447\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.015009460598230362.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5807 - accuracy: 0.9237 - val_loss: 1.0891 - val_accuracy: 0.7867 - lr: 0.0167\n",
            "Epoch 110/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.9245\n",
            "Epoch 00110: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5576 - accuracy: 0.9245 - val_loss: 0.7121 - val_accuracy: 0.8891 - lr: 0.0150\n",
            "Epoch 111/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.9253\n",
            "Epoch 00111: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5526 - accuracy: 0.9253 - val_loss: 1.6302 - val_accuracy: 0.6579 - lr: 0.0150\n",
            "Epoch 112/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.9148\n",
            "Epoch 00112: val_accuracy did not improve from 0.91447\n",
            "\n",
            "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.013508514873683453.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5900 - accuracy: 0.9148 - val_loss: 1.1285 - val_accuracy: 0.7709 - lr: 0.0150\n",
            "Epoch 113/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.9261\n",
            "Epoch 00113: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5440 - accuracy: 0.9261 - val_loss: 0.7104 - val_accuracy: 0.8743 - lr: 0.0135\n",
            "Epoch 114/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.9258\n",
            "Epoch 00114: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5531 - accuracy: 0.9258 - val_loss: 0.9814 - val_accuracy: 0.8078 - lr: 0.0135\n",
            "Epoch 115/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.9319\n",
            "Epoch 00115: val_accuracy did not improve from 0.91447\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5423 - accuracy: 0.9319 - val_loss: 0.6514 - val_accuracy: 0.9007 - lr: 0.0135\n",
            "Epoch 116/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.9400\n",
            "Epoch 00116: val_accuracy improved from 0.91447 to 0.91658, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 73s 1s/step - loss: 0.5043 - accuracy: 0.9400 - val_loss: 0.6370 - val_accuracy: 0.9166 - lr: 0.0135\n",
            "Epoch 117/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.9361\n",
            "Epoch 00117: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5088 - accuracy: 0.9361 - val_loss: 1.3803 - val_accuracy: 0.6948 - lr: 0.0135\n",
            "Epoch 118/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.9272\n",
            "Epoch 00118: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5237 - accuracy: 0.9272 - val_loss: 1.2070 - val_accuracy: 0.7223 - lr: 0.0135\n",
            "Epoch 119/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.9290\n",
            "Epoch 00119: val_accuracy did not improve from 0.91658\n",
            "\n",
            "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.012157663051038981.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5351 - accuracy: 0.9290 - val_loss: 1.7509 - val_accuracy: 0.6515 - lr: 0.0135\n",
            "Epoch 120/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.9351\n",
            "Epoch 00120: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4970 - accuracy: 0.9351 - val_loss: 3.1660 - val_accuracy: 0.4646 - lr: 0.0122\n",
            "Epoch 121/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.9353\n",
            "Epoch 00121: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.5018 - accuracy: 0.9353 - val_loss: 1.1176 - val_accuracy: 0.7645 - lr: 0.0122\n",
            "Epoch 122/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.9343\n",
            "Epoch 00122: val_accuracy did not improve from 0.91658\n",
            "\n",
            "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.010941896494477988.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.5102 - accuracy: 0.9343 - val_loss: 2.5755 - val_accuracy: 0.5164 - lr: 0.0122\n",
            "Epoch 123/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.9364\n",
            "Epoch 00123: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4913 - accuracy: 0.9364 - val_loss: 0.6344 - val_accuracy: 0.9081 - lr: 0.0109\n",
            "Epoch 124/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.9435\n",
            "Epoch 00124: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4747 - accuracy: 0.9435 - val_loss: 1.1682 - val_accuracy: 0.7719 - lr: 0.0109\n",
            "Epoch 125/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.9329\n",
            "Epoch 00125: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4908 - accuracy: 0.9329 - val_loss: 0.8098 - val_accuracy: 0.8321 - lr: 0.0109\n",
            "Epoch 126/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4641 - accuracy: 0.9479\n",
            "Epoch 00126: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4641 - accuracy: 0.9479 - val_loss: 0.6076 - val_accuracy: 0.9124 - lr: 0.0109\n",
            "Epoch 127/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.9361\n",
            "Epoch 00127: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4839 - accuracy: 0.9361 - val_loss: 0.6039 - val_accuracy: 0.9060 - lr: 0.0109\n",
            "Epoch 128/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.9390\n",
            "Epoch 00128: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4679 - accuracy: 0.9390 - val_loss: 0.6845 - val_accuracy: 0.8870 - lr: 0.0109\n",
            "Epoch 129/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.9403\n",
            "Epoch 00129: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4777 - accuracy: 0.9403 - val_loss: 1.8683 - val_accuracy: 0.6473 - lr: 0.0109\n",
            "Epoch 130/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.9361\n",
            "Epoch 00130: val_accuracy did not improve from 0.91658\n",
            "\n",
            "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.009847706928849221.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4741 - accuracy: 0.9361 - val_loss: 1.0050 - val_accuracy: 0.7730 - lr: 0.0109\n",
            "Epoch 131/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.9351\n",
            "Epoch 00131: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4712 - accuracy: 0.9351 - val_loss: 0.6743 - val_accuracy: 0.8733 - lr: 0.0098\n",
            "Epoch 132/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.9408\n",
            "Epoch 00132: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4621 - accuracy: 0.9408 - val_loss: 1.4345 - val_accuracy: 0.7297 - lr: 0.0098\n",
            "Epoch 133/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.9364\n",
            "Epoch 00133: val_accuracy did not improve from 0.91658\n",
            "\n",
            "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.008862936403602362.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4672 - accuracy: 0.9364 - val_loss: 0.8456 - val_accuracy: 0.8110 - lr: 0.0098\n",
            "Epoch 134/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.9377\n",
            "Epoch 00134: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4585 - accuracy: 0.9377 - val_loss: 0.6890 - val_accuracy: 0.8817 - lr: 0.0089\n",
            "Epoch 135/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.9377\n",
            "Epoch 00135: val_accuracy did not improve from 0.91658\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4596 - accuracy: 0.9377 - val_loss: 0.7243 - val_accuracy: 0.8701 - lr: 0.0089\n",
            "Epoch 136/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.9461\n",
            "Epoch 00136: val_accuracy did not improve from 0.91658\n",
            "\n",
            "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.007976643182337284.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4348 - accuracy: 0.9461 - val_loss: 0.8823 - val_accuracy: 0.8173 - lr: 0.0089\n",
            "Epoch 137/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.9395\n",
            "Epoch 00137: val_accuracy improved from 0.91658 to 0.91869, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.4535 - accuracy: 0.9395 - val_loss: 0.5636 - val_accuracy: 0.9187 - lr: 0.0080\n",
            "Epoch 138/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.9445\n",
            "Epoch 00138: val_accuracy did not improve from 0.91869\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4375 - accuracy: 0.9445 - val_loss: 0.7005 - val_accuracy: 0.8775 - lr: 0.0080\n",
            "Epoch 139/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.9461\n",
            "Epoch 00139: val_accuracy did not improve from 0.91869\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4301 - accuracy: 0.9461 - val_loss: 0.5957 - val_accuracy: 0.9039 - lr: 0.0080\n",
            "Epoch 140/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.9453\n",
            "Epoch 00140: val_accuracy did not improve from 0.91869\n",
            "\n",
            "Epoch 00140: ReduceLROnPlateau reducing learning rate to 0.007178978528827429.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4384 - accuracy: 0.9453 - val_loss: 0.8986 - val_accuracy: 0.8184 - lr: 0.0080\n",
            "Epoch 141/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4168 - accuracy: 0.9524\n",
            "Epoch 00141: val_accuracy did not improve from 0.91869\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4168 - accuracy: 0.9524 - val_loss: 0.5915 - val_accuracy: 0.9134 - lr: 0.0072\n",
            "Epoch 142/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.9519\n",
            "Epoch 00142: val_accuracy did not improve from 0.91869\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.4065 - accuracy: 0.9519 - val_loss: 0.5753 - val_accuracy: 0.9102 - lr: 0.0072\n",
            "Epoch 143/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4092 - accuracy: 0.9471\n",
            "Epoch 00143: val_accuracy did not improve from 0.91869\n",
            "\n",
            "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.006461080675944686.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4092 - accuracy: 0.9471 - val_loss: 0.7812 - val_accuracy: 0.8405 - lr: 0.0072\n",
            "Epoch 144/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.9527\n",
            "Epoch 00144: val_accuracy did not improve from 0.91869\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4000 - accuracy: 0.9527 - val_loss: 0.6174 - val_accuracy: 0.8997 - lr: 0.0065\n",
            "Epoch 145/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.9506\n",
            "Epoch 00145: val_accuracy improved from 0.91869 to 0.93770, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.4124 - accuracy: 0.9506 - val_loss: 0.5091 - val_accuracy: 0.9377 - lr: 0.0065\n",
            "Epoch 146/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.9487\n",
            "Epoch 00146: val_accuracy did not improve from 0.93770\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.4126 - accuracy: 0.9487 - val_loss: 0.5477 - val_accuracy: 0.9134 - lr: 0.0065\n",
            "Epoch 147/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.9537\n",
            "Epoch 00147: val_accuracy did not improve from 0.93770\n",
            "60/60 [==============================] - 66s 1s/step - loss: 0.4032 - accuracy: 0.9537 - val_loss: 0.5091 - val_accuracy: 0.9314 - lr: 0.0065\n",
            "Epoch 148/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.9566\n",
            "Epoch 00148: val_accuracy did not improve from 0.93770\n",
            "\n",
            "Epoch 00148: ReduceLROnPlateau reducing learning rate to 0.0058149725664407015.\n",
            "60/60 [==============================] - 66s 1s/step - loss: 0.3922 - accuracy: 0.9566 - val_loss: 0.6170 - val_accuracy: 0.9092 - lr: 0.0065\n",
            "Epoch 149/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.9503\n",
            "Epoch 00149: val_accuracy did not improve from 0.93770\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3994 - accuracy: 0.9503 - val_loss: 0.4848 - val_accuracy: 0.9335 - lr: 0.0058\n",
            "Epoch 150/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3919 - accuracy: 0.9516\n",
            "Epoch 00150: val_accuracy did not improve from 0.93770\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3919 - accuracy: 0.9516 - val_loss: 0.9165 - val_accuracy: 0.8131 - lr: 0.0058\n",
            "Epoch 151/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.9608\n",
            "Epoch 00151: val_accuracy did not improve from 0.93770\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3758 - accuracy: 0.9608 - val_loss: 0.5058 - val_accuracy: 0.9271 - lr: 0.0058\n",
            "Epoch 152/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.9535\n",
            "Epoch 00152: val_accuracy did not improve from 0.93770\n",
            "\n",
            "Epoch 00152: ReduceLROnPlateau reducing learning rate to 0.005233475100249053.\n",
            "60/60 [==============================] - 66s 1s/step - loss: 0.3890 - accuracy: 0.9535 - val_loss: 0.5376 - val_accuracy: 0.9145 - lr: 0.0058\n",
            "Epoch 153/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3782 - accuracy: 0.9550\n",
            "Epoch 00153: val_accuracy improved from 0.93770 to 0.94403, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.3782 - accuracy: 0.9550 - val_loss: 0.4595 - val_accuracy: 0.9440 - lr: 0.0052\n",
            "Epoch 154/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.9524\n",
            "Epoch 00154: val_accuracy did not improve from 0.94403\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3863 - accuracy: 0.9524 - val_loss: 0.4867 - val_accuracy: 0.9409 - lr: 0.0052\n",
            "Epoch 155/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.9542\n",
            "Epoch 00155: val_accuracy did not improve from 0.94403\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3841 - accuracy: 0.9542 - val_loss: 0.4804 - val_accuracy: 0.9419 - lr: 0.0052\n",
            "Epoch 156/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.9574\n",
            "Epoch 00156: val_accuracy did not improve from 0.94403\n",
            "\n",
            "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0047101275064051155.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3666 - accuracy: 0.9574 - val_loss: 0.4679 - val_accuracy: 0.9388 - lr: 0.0052\n",
            "Epoch 157/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.9590\n",
            "Epoch 00157: val_accuracy improved from 0.94403 to 0.94826, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.3681 - accuracy: 0.9590 - val_loss: 0.4629 - val_accuracy: 0.9483 - lr: 0.0047\n",
            "Epoch 158/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3592 - accuracy: 0.9613\n",
            "Epoch 00158: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3592 - accuracy: 0.9613 - val_loss: 0.4574 - val_accuracy: 0.9483 - lr: 0.0047\n",
            "Epoch 159/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3633 - accuracy: 0.9640\n",
            "Epoch 00159: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3633 - accuracy: 0.9640 - val_loss: 0.6127 - val_accuracy: 0.8891 - lr: 0.0047\n",
            "Epoch 160/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.9566\n",
            "Epoch 00160: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3656 - accuracy: 0.9566 - val_loss: 0.4541 - val_accuracy: 0.9483 - lr: 0.0047\n",
            "Epoch 161/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.9542\n",
            "Epoch 00161: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3788 - accuracy: 0.9542 - val_loss: 0.8171 - val_accuracy: 0.8258 - lr: 0.0047\n",
            "Epoch 162/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.9587\n",
            "Epoch 00162: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3727 - accuracy: 0.9587 - val_loss: 0.5480 - val_accuracy: 0.9261 - lr: 0.0047\n",
            "Epoch 163/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.9598\n",
            "Epoch 00163: val_accuracy did not improve from 0.94826\n",
            "\n",
            "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.004239114839583636.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3594 - accuracy: 0.9598 - val_loss: 0.5459 - val_accuracy: 0.9155 - lr: 0.0047\n",
            "Epoch 164/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.9558\n",
            "Epoch 00164: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3582 - accuracy: 0.9558 - val_loss: 0.4573 - val_accuracy: 0.9461 - lr: 0.0042\n",
            "Epoch 165/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.9603\n",
            "Epoch 00165: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3471 - accuracy: 0.9603 - val_loss: 0.5091 - val_accuracy: 0.9271 - lr: 0.0042\n",
            "Epoch 166/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3616 - accuracy: 0.9564\n",
            "Epoch 00166: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3616 - accuracy: 0.9564 - val_loss: 0.4385 - val_accuracy: 0.9472 - lr: 0.0042\n",
            "Epoch 167/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.9645\n",
            "Epoch 00167: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3431 - accuracy: 0.9645 - val_loss: 0.6202 - val_accuracy: 0.8923 - lr: 0.0042\n",
            "Epoch 168/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.9621\n",
            "Epoch 00168: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3540 - accuracy: 0.9621 - val_loss: 0.4654 - val_accuracy: 0.9398 - lr: 0.0042\n",
            "Epoch 169/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9645\n",
            "Epoch 00169: val_accuracy did not improve from 0.94826\n",
            "\n",
            "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0038152034394443035.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3470 - accuracy: 0.9645 - val_loss: 0.5209 - val_accuracy: 0.9208 - lr: 0.0042\n",
            "Epoch 170/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.9603\n",
            "Epoch 00170: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3507 - accuracy: 0.9603 - val_loss: 0.5915 - val_accuracy: 0.8912 - lr: 0.0038\n",
            "Epoch 171/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.9616\n",
            "Epoch 00171: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3477 - accuracy: 0.9616 - val_loss: 0.4675 - val_accuracy: 0.9419 - lr: 0.0038\n",
            "Epoch 172/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.9640\n",
            "Epoch 00172: val_accuracy did not improve from 0.94826\n",
            "\n",
            "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.003433683095499873.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3474 - accuracy: 0.9640 - val_loss: 0.5036 - val_accuracy: 0.9335 - lr: 0.0038\n",
            "Epoch 173/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9653\n",
            "Epoch 00173: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3410 - accuracy: 0.9653 - val_loss: 0.5951 - val_accuracy: 0.9007 - lr: 0.0034\n",
            "Epoch 174/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.9674\n",
            "Epoch 00174: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3292 - accuracy: 0.9674 - val_loss: 0.5715 - val_accuracy: 0.9018 - lr: 0.0034\n",
            "Epoch 175/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.9629\n",
            "Epoch 00175: val_accuracy did not improve from 0.94826\n",
            "\n",
            "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0030903148697689177.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3435 - accuracy: 0.9629 - val_loss: 0.4404 - val_accuracy: 0.9472 - lr: 0.0034\n",
            "Epoch 176/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.9621\n",
            "Epoch 00176: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3363 - accuracy: 0.9621 - val_loss: 0.4597 - val_accuracy: 0.9419 - lr: 0.0031\n",
            "Epoch 177/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.9650\n",
            "Epoch 00177: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3390 - accuracy: 0.9650 - val_loss: 0.5366 - val_accuracy: 0.9145 - lr: 0.0031\n",
            "Epoch 178/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.9640\n",
            "Epoch 00178: val_accuracy did not improve from 0.94826\n",
            "\n",
            "Epoch 00178: ReduceLROnPlateau reducing learning rate to 0.002781283319927752.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3369 - accuracy: 0.9640 - val_loss: 0.4819 - val_accuracy: 0.9271 - lr: 0.0031\n",
            "Epoch 179/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.9629\n",
            "Epoch 00179: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3401 - accuracy: 0.9629 - val_loss: 0.4753 - val_accuracy: 0.9430 - lr: 0.0028\n",
            "Epoch 180/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9663\n",
            "Epoch 00180: val_accuracy did not improve from 0.94826\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3282 - accuracy: 0.9663 - val_loss: 0.4508 - val_accuracy: 0.9398 - lr: 0.0028\n",
            "Epoch 181/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.9663\n",
            "Epoch 00181: val_accuracy improved from 0.94826 to 0.96093, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.3250 - accuracy: 0.9663 - val_loss: 0.4188 - val_accuracy: 0.9609 - lr: 0.0028\n",
            "Epoch 182/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.9600\n",
            "Epoch 00182: val_accuracy did not improve from 0.96093\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3485 - accuracy: 0.9600 - val_loss: 0.4280 - val_accuracy: 0.9451 - lr: 0.0028\n",
            "Epoch 183/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.9684\n",
            "Epoch 00183: val_accuracy improved from 0.96093 to 0.96410, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.3268 - accuracy: 0.9684 - val_loss: 0.4050 - val_accuracy: 0.9641 - lr: 0.0028\n",
            "Epoch 184/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9637\n",
            "Epoch 00184: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3294 - accuracy: 0.9637 - val_loss: 0.4300 - val_accuracy: 0.9578 - lr: 0.0028\n",
            "Epoch 185/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9650\n",
            "Epoch 00185: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3221 - accuracy: 0.9650 - val_loss: 0.4032 - val_accuracy: 0.9588 - lr: 0.0028\n",
            "Epoch 186/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.9669\n",
            "Epoch 00186: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3246 - accuracy: 0.9669 - val_loss: 0.4396 - val_accuracy: 0.9493 - lr: 0.0028\n",
            "Epoch 187/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9684\n",
            "Epoch 00187: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3226 - accuracy: 0.9684 - val_loss: 0.4698 - val_accuracy: 0.9388 - lr: 0.0028\n",
            "Epoch 188/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.9677\n",
            "Epoch 00188: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00188: ReduceLROnPlateau reducing learning rate to 0.002503155008889735.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3139 - accuracy: 0.9677 - val_loss: 0.4395 - val_accuracy: 0.9440 - lr: 0.0028\n",
            "Epoch 189/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.9619\n",
            "Epoch 00189: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3312 - accuracy: 0.9619 - val_loss: 0.4819 - val_accuracy: 0.9250 - lr: 0.0025\n",
            "Epoch 190/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.9695\n",
            "Epoch 00190: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3126 - accuracy: 0.9695 - val_loss: 0.4185 - val_accuracy: 0.9525 - lr: 0.0025\n",
            "Epoch 191/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.9650\n",
            "Epoch 00191: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 0.002252839528955519.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3264 - accuracy: 0.9650 - val_loss: 0.4295 - val_accuracy: 0.9472 - lr: 0.0025\n",
            "Epoch 192/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9653\n",
            "Epoch 00192: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3294 - accuracy: 0.9653 - val_loss: 0.4262 - val_accuracy: 0.9525 - lr: 0.0023\n",
            "Epoch 193/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9716\n",
            "Epoch 00193: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3111 - accuracy: 0.9716 - val_loss: 0.5103 - val_accuracy: 0.9303 - lr: 0.0023\n",
            "Epoch 194/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9705\n",
            "Epoch 00194: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00194: ReduceLROnPlateau reducing learning rate to 0.0020275555551052095.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3102 - accuracy: 0.9705 - val_loss: 0.4764 - val_accuracy: 0.9303 - lr: 0.0023\n",
            "Epoch 195/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9698\n",
            "Epoch 00195: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3095 - accuracy: 0.9698 - val_loss: 0.4182 - val_accuracy: 0.9483 - lr: 0.0020\n",
            "Epoch 196/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9684\n",
            "Epoch 00196: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3123 - accuracy: 0.9684 - val_loss: 0.3996 - val_accuracy: 0.9609 - lr: 0.0020\n",
            "Epoch 197/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9687\n",
            "Epoch 00197: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3125 - accuracy: 0.9687 - val_loss: 0.4116 - val_accuracy: 0.9567 - lr: 0.0020\n",
            "Epoch 198/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9671\n",
            "Epoch 00198: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3127 - accuracy: 0.9671 - val_loss: 0.4302 - val_accuracy: 0.9535 - lr: 0.0020\n",
            "Epoch 199/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9684\n",
            "Epoch 00199: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00199: ReduceLROnPlateau reducing learning rate to 0.0018248000415042043.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3078 - accuracy: 0.9684 - val_loss: 0.4605 - val_accuracy: 0.9388 - lr: 0.0020\n",
            "Epoch 200/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.9695\n",
            "Epoch 00200: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3084 - accuracy: 0.9695 - val_loss: 0.4433 - val_accuracy: 0.9398 - lr: 0.0018\n",
            "Epoch 201/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.9729\n",
            "Epoch 00201: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2986 - accuracy: 0.9729 - val_loss: 0.4030 - val_accuracy: 0.9525 - lr: 0.0018\n",
            "Epoch 202/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9677\n",
            "Epoch 00202: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.001642320037353784.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3095 - accuracy: 0.9677 - val_loss: 0.4414 - val_accuracy: 0.9366 - lr: 0.0018\n",
            "Epoch 203/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9687\n",
            "Epoch 00203: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3074 - accuracy: 0.9687 - val_loss: 0.4144 - val_accuracy: 0.9461 - lr: 0.0016\n",
            "Epoch 204/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.9703\n",
            "Epoch 00204: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3022 - accuracy: 0.9703 - val_loss: 0.4023 - val_accuracy: 0.9525 - lr: 0.0016\n",
            "Epoch 205/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9692\n",
            "Epoch 00205: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3040 - accuracy: 0.9692 - val_loss: 0.3868 - val_accuracy: 0.9599 - lr: 0.0016\n",
            "Epoch 206/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.9729\n",
            "Epoch 00206: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2989 - accuracy: 0.9729 - val_loss: 0.4239 - val_accuracy: 0.9409 - lr: 0.0016\n",
            "Epoch 207/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.9695\n",
            "Epoch 00207: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3049 - accuracy: 0.9695 - val_loss: 0.4128 - val_accuracy: 0.9514 - lr: 0.0016\n",
            "Epoch 208/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9719\n",
            "Epoch 00208: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00208: ReduceLROnPlateau reducing learning rate to 0.0014780880650505424.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3009 - accuracy: 0.9719 - val_loss: 0.3998 - val_accuracy: 0.9609 - lr: 0.0016\n",
            "Epoch 209/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.9700\n",
            "Epoch 00209: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.3012 - accuracy: 0.9700 - val_loss: 0.3879 - val_accuracy: 0.9578 - lr: 0.0015\n",
            "Epoch 210/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9745\n",
            "Epoch 00210: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2942 - accuracy: 0.9745 - val_loss: 0.3959 - val_accuracy: 0.9588 - lr: 0.0015\n",
            "Epoch 211/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9724\n",
            "Epoch 00211: val_accuracy did not improve from 0.96410\n",
            "\n",
            "Epoch 00211: ReduceLROnPlateau reducing learning rate to 0.001330279279500246.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2948 - accuracy: 0.9724 - val_loss: 0.3992 - val_accuracy: 0.9556 - lr: 0.0015\n",
            "Epoch 212/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.9716\n",
            "Epoch 00212: val_accuracy did not improve from 0.96410\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.3014 - accuracy: 0.9716 - val_loss: 0.3868 - val_accuracy: 0.9630 - lr: 0.0013\n",
            "Epoch 213/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9734\n",
            "Epoch 00213: val_accuracy improved from 0.96410 to 0.96938, saving model to best_model_vgg16_plant_seedlings\n",
            "INFO:tensorflow:Assets written to: best_model_vgg16_plant_seedlings/assets\n",
            "60/60 [==============================] - 72s 1s/step - loss: 0.2899 - accuracy: 0.9734 - val_loss: 0.3643 - val_accuracy: 0.9694 - lr: 0.0013\n",
            "Epoch 214/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9716\n",
            "Epoch 00214: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2962 - accuracy: 0.9716 - val_loss: 0.3837 - val_accuracy: 0.9599 - lr: 0.0013\n",
            "Epoch 215/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9692\n",
            "Epoch 00215: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2961 - accuracy: 0.9692 - val_loss: 0.4670 - val_accuracy: 0.9377 - lr: 0.0013\n",
            "Epoch 216/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9742\n",
            "Epoch 00216: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00216: ReduceLROnPlateau reducing learning rate to 0.0011972513515502215.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2901 - accuracy: 0.9742 - val_loss: 0.3857 - val_accuracy: 0.9525 - lr: 0.0013\n",
            "Epoch 217/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9753\n",
            "Epoch 00217: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2829 - accuracy: 0.9753 - val_loss: 0.3690 - val_accuracy: 0.9641 - lr: 0.0012\n",
            "Epoch 218/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9774\n",
            "Epoch 00218: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2833 - accuracy: 0.9774 - val_loss: 0.3831 - val_accuracy: 0.9556 - lr: 0.0012\n",
            "Epoch 219/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9711\n",
            "Epoch 00219: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00219: ReduceLROnPlateau reducing learning rate to 0.0010775262373499573.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2875 - accuracy: 0.9711 - val_loss: 0.4140 - val_accuracy: 0.9388 - lr: 0.0012\n",
            "Epoch 220/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9784\n",
            "Epoch 00220: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2742 - accuracy: 0.9784 - val_loss: 0.3932 - val_accuracy: 0.9514 - lr: 0.0011\n",
            "Epoch 221/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.9774\n",
            "Epoch 00221: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2817 - accuracy: 0.9774 - val_loss: 0.4160 - val_accuracy: 0.9567 - lr: 0.0011\n",
            "Epoch 222/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9713\n",
            "Epoch 00222: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00222: ReduceLROnPlateau reducing learning rate to 0.0009697736240923405.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2962 - accuracy: 0.9713 - val_loss: 0.3853 - val_accuracy: 0.9588 - lr: 0.0011\n",
            "Epoch 223/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9784\n",
            "Epoch 00223: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2833 - accuracy: 0.9784 - val_loss: 0.3646 - val_accuracy: 0.9694 - lr: 9.6977e-04\n",
            "Epoch 224/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.9703\n",
            "Epoch 00224: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2933 - accuracy: 0.9703 - val_loss: 0.3920 - val_accuracy: 0.9662 - lr: 9.6977e-04\n",
            "Epoch 225/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9784\n",
            "Epoch 00225: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00225: ReduceLROnPlateau reducing learning rate to 0.0008727962616831064.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2806 - accuracy: 0.9784 - val_loss: 0.3674 - val_accuracy: 0.9567 - lr: 9.6977e-04\n",
            "Epoch 226/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.9753\n",
            "Epoch 00226: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2946 - accuracy: 0.9753 - val_loss: 0.3909 - val_accuracy: 0.9609 - lr: 8.7280e-04\n",
            "Epoch 227/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.9703\n",
            "Epoch 00227: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2961 - accuracy: 0.9703 - val_loss: 0.4041 - val_accuracy: 0.9567 - lr: 8.7280e-04\n",
            "Epoch 228/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9748\n",
            "Epoch 00228: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00228: ReduceLROnPlateau reducing learning rate to 0.0007855166564695537.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2849 - accuracy: 0.9748 - val_loss: 0.3878 - val_accuracy: 0.9546 - lr: 8.7280e-04\n",
            "Epoch 229/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9724\n",
            "Epoch 00229: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2872 - accuracy: 0.9724 - val_loss: 0.3976 - val_accuracy: 0.9546 - lr: 7.8552e-04\n",
            "Epoch 230/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9721\n",
            "Epoch 00230: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2903 - accuracy: 0.9721 - val_loss: 0.4648 - val_accuracy: 0.9366 - lr: 7.8552e-04\n",
            "Epoch 231/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9779\n",
            "Epoch 00231: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00231: ReduceLROnPlateau reducing learning rate to 0.0007069649698678405.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2790 - accuracy: 0.9779 - val_loss: 0.4165 - val_accuracy: 0.9493 - lr: 7.8552e-04\n",
            "Epoch 232/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9790\n",
            "Epoch 00232: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2804 - accuracy: 0.9790 - val_loss: 0.3937 - val_accuracy: 0.9599 - lr: 7.0696e-04\n",
            "Epoch 233/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9719\n",
            "Epoch 00233: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2853 - accuracy: 0.9719 - val_loss: 0.3737 - val_accuracy: 0.9599 - lr: 7.0696e-04\n",
            "Epoch 234/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.9774\n",
            "Epoch 00234: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00234: ReduceLROnPlateau reducing learning rate to 0.0006362684885971248.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2781 - accuracy: 0.9774 - val_loss: 0.3817 - val_accuracy: 0.9630 - lr: 7.0696e-04\n",
            "Epoch 235/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9771\n",
            "Epoch 00235: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2714 - accuracy: 0.9771 - val_loss: 0.3713 - val_accuracy: 0.9641 - lr: 6.3627e-04\n",
            "Epoch 236/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9761\n",
            "Epoch 00236: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2754 - accuracy: 0.9761 - val_loss: 0.3853 - val_accuracy: 0.9567 - lr: 6.3627e-04\n",
            "Epoch 237/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.9729\n",
            "Epoch 00237: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00237: ReduceLROnPlateau reducing learning rate to 0.0005726416187826544.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2827 - accuracy: 0.9729 - val_loss: 0.4201 - val_accuracy: 0.9483 - lr: 6.3627e-04\n",
            "Epoch 238/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9740\n",
            "Epoch 00238: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2822 - accuracy: 0.9740 - val_loss: 0.3659 - val_accuracy: 0.9578 - lr: 5.7264e-04\n",
            "Epoch 239/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.9805\n",
            "Epoch 00239: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2690 - accuracy: 0.9805 - val_loss: 0.3698 - val_accuracy: 0.9620 - lr: 5.7264e-04\n",
            "Epoch 240/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9819\n",
            "Epoch 00240: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00240: ReduceLROnPlateau reducing learning rate to 0.0005153774516656995.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2755 - accuracy: 0.9819 - val_loss: 0.3737 - val_accuracy: 0.9641 - lr: 5.7264e-04\n",
            "Epoch 241/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9790\n",
            "Epoch 00241: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2696 - accuracy: 0.9790 - val_loss: 0.4119 - val_accuracy: 0.9504 - lr: 5.1538e-04\n",
            "Epoch 242/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9769\n",
            "Epoch 00242: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2800 - accuracy: 0.9769 - val_loss: 0.3726 - val_accuracy: 0.9599 - lr: 5.1538e-04\n",
            "Epoch 243/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.9774\n",
            "Epoch 00243: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00243: ReduceLROnPlateau reducing learning rate to 0.00046383969602175056.\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2738 - accuracy: 0.9774 - val_loss: 0.3738 - val_accuracy: 0.9620 - lr: 5.1538e-04\n",
            "Epoch 244/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9805\n",
            "Epoch 00244: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2696 - accuracy: 0.9805 - val_loss: 0.3968 - val_accuracy: 0.9546 - lr: 4.6384e-04\n",
            "Epoch 245/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9774\n",
            "Epoch 00245: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2831 - accuracy: 0.9774 - val_loss: 0.3819 - val_accuracy: 0.9609 - lr: 4.6384e-04\n",
            "Epoch 246/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9769\n",
            "Epoch 00246: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00246: ReduceLROnPlateau reducing learning rate to 0.0004174557368969545.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2734 - accuracy: 0.9769 - val_loss: 0.3905 - val_accuracy: 0.9609 - lr: 4.6384e-04\n",
            "Epoch 247/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9795\n",
            "Epoch 00247: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2668 - accuracy: 0.9795 - val_loss: 0.3790 - val_accuracy: 0.9641 - lr: 4.1746e-04\n",
            "Epoch 248/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9811\n",
            "Epoch 00248: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2710 - accuracy: 0.9811 - val_loss: 0.3742 - val_accuracy: 0.9578 - lr: 4.1746e-04\n",
            "Epoch 249/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9774\n",
            "Epoch 00249: val_accuracy did not improve from 0.96938\n",
            "\n",
            "Epoch 00249: ReduceLROnPlateau reducing learning rate to 0.00037571016582660377.\n",
            "60/60 [==============================] - 67s 1s/step - loss: 0.2763 - accuracy: 0.9774 - val_loss: 0.3747 - val_accuracy: 0.9630 - lr: 4.1746e-04\n",
            "Epoch 250/250\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9774\n",
            "Epoch 00250: val_accuracy did not improve from 0.96938\n",
            "60/60 [==============================] - 68s 1s/step - loss: 0.2725 - accuracy: 0.9774 - val_loss: 0.3591 - val_accuracy: 0.9652 - lr: 3.7571e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "0iMb8pSI_ICn",
        "outputId": "11256839-d34b-4219-84e8-4b4d618ba757"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history.history['val_loss'],label='Test loss')\n",
        "plt.plot(history.history['loss'],label='Train loss')\n",
        "plt.title('Loss curve for improved vgg16 model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history.history['val_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history.history['accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for improved vgg16 model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAGDCAYAAADETHGkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAArvUlEQVR4nO3de7hlVX3m+++7qkCMoIBUOHJRiBI7aHsLIsYcQ8TDLUaIUQNtB2ITMS1ojEkM2DkH46UTE40Jp700JyDgraQxPpAIQYKosROQ8i4YpOQihSilxUUFQajf+WOOXbXY7l21oWquNdn7+3me9ew1x7yNNfai9suYY8yZqkKSJGmpGE27ApIkSZNk+JEkSUuK4UeSJC0phh9JkrSkGH4kSdKSYviRJElLiuFH0oOW5DeS3Jjkh0mevhWOd2GSY7dG3YYiyYFJ1ky7HkP1QNonyRuTfKDvOmnxM/xID0CS65M8f9r1GJC3AydW1fZV9cUtPVhVHVZVZ22Fei15SX41yaVJbk9y/Tzb/H6S65L8KMnXk/z8hKspTYXhR1pCkizfyod8HHDlg6zLsq1clwdy7q3dDkP0I+AM4I/nWpnkd4HjgF8DtgdeAHxvYrWTpsjwI20FSR6W5G+SfLu9/ibJw9q6XZL8Y5LbkqxL8i9JRm3dnyS5KckPklyd5KB5jv/wJO9IckP7P/nPtrKfumQw3jvVLhOcm+QDSe4A3pDkriQ7j23/9CTfS7JNW/4vrRfg1iQXJXncPJ/3h8Ay4MtJvtnKfyHJp9pnvTLJC8f2OTPJe5JckORHwK/OcdxPtT/KJPmdJP87yTvb8a5N8kut/MYkt4xfImvHf2+Si1t7fnq87kkqyQlJrgGuaWWvSLK6/V7OT7JbK39PkrfPqtt5SV7X3u+W5KNJ1raek9fM+l2d2drvKuCZc/1OF3ieZyT5Yvs8/yvJR5K8ZWzb1ye5uX3nfrd9xicAVNXnqur9wLVznHcEnAL8QVVdVZ1vVtW6eer5xnb+D7S6fDXJzyc5uf0ebkxy8Nj2u7X2XNfa9xULbZ9Nta20tRh+pK3jvwEHAE8DngrsD/xpW/eHwBpgBbAr8AagkjwROBF4ZlXtABwCXD/P8d8O/CLwS8DOwOuB9Qus2xHAucCOwF8B/wb85tj6/wScW1U/SXJEq9+LWn3/Bfjw7ANW1d1VtX1bfGpVPb6Fp38APgH8LPBq4IPtc46f663ADsBnF1D3ZwFfAR4NfAhYSffH8gnAfwb+R5Ltx7Z/GfBmYBfgS8AHZx3vyHbMfZM8D/hz4KXAY4Ab2vFpn/m3kgQgyU7AwcDKFhz+AfgysDtwEPDaJIe0fU8BHt9ehwCbGsO0qfNsC3wMOJPud/5h4DdmdkxyKPA64PmtPQ7cxHlm26O9ntyCy3VJ/qx9tvn8OvB+YCfgi8BFdH9DdgfeBPzPsW1X0n3ndwNeDPz31t6wifZZQNtKW0dV+fLla4EvunDy/DnKvwkcPrZ8CHB9e/8m4DzgCbP2eQJwC90fr202cc4RcBddyJi97kBgzXx1BN4IfGbW+t8FPtneB7gReG5bvhA4bta57wQeN0/dauZzAf8n8B1gNLb+w8Ab2/szgbM3076fAn63vf8d4Jqxdf+xnW/XsbLvA08bO/7KsXXbA/cBe47V9Xlj608H/nLW9j8B9mrt8q2xdnnFWJs9C/jWrHqfDLyvvb8WOHRs3fGzf0dj6zZ1nucCNwEZ2/6zwFva+zOAP5/1fdrw+xgrfz7tuzhW9ktt24/TheK9gG8Ar5innm8ELh5b/nXgh8CytrxDO96OwJ6t3XcY2/7PgTM31z4LaNs3Ah/Y2v9d+1p6L3t+pK1jN7qegxk3tDLoeltWA59ol25OAqiq1cBr6f5BvyXJypnLLrPsAmxHF7AejBtnLX8UeHaSx9D9gV1P18MD3Riev22XmW4D1tH9gd59AefZDbixqsZ7pG6Yte/sumzOd8fe3wVQVbPLxnt+Nhy/qn5IV//d5lrPrN9Z2/77wO5VVXS9F0e31f+Jjb1IjwN2m2mj1k5voOvVmznu+HnGvxf3s5nz7Abc1LaZr/43zrNuc+5qP/+yqm6rquvpem4O38Q+s9v9e1V136zjbd/qta6qfjC2/fj3YFPts7m2lbYKw4+0dXyb7h/uGY9tZVTVD6rqD6vq54AXAq9LG9tTVR+qql9u+xbwtjmO/T3gx3SXCWb7EfAzMwvpBhGvmLVN3W+h6la6S1O/RffHduXYH9gbgVdW1Y5jr4dX1b9utgW6z7vnrEsnj6XrvZizLj3Yc+ZNuxy2c6vXXOe/3+8sySPoLq/N1PfDwIvbuKFn0YVG6NroullttENVzQSHm8frQdcGmzLfeW4Gdp+5JDb787X1e8yzbnOuBu7h/u2xtX433wZ2TrLDWNn492BT7bO5tpW2CsOP9MBtk2S7sddyuj9gf5pkRZJdgP8H+ABAkhckeUL7I3Y73SWB9UmemOR56QZG/5ju/55/ahxP60k5A/jrNhh0WZJnt/2+AWyX5NfamJs/BR62gM/wIeAYuvEYHxorfy9wcpIntbo/KslLFtgul9NdInt9km2SHEh3eWTlpnbayg5P8sttvMybgcuqar4ekQ8DL0/ytNaW/x24vPWCUN3U/e8BfwdcVFW3tf0+B/wg3WD1h7ffx5OTzAzcPYeuDXdKsgfd2Kd5beI8/0b3XTkxyfI2Hmv/sV3PafX/hSQ/A/zf48dNMkqyHbBNt5jtWrtQVXcCH6H7Xe3Q6nk88I+bqutCtPb+V+DP2zmfQjerbOb+PJtqn821rbRVGH6kB+4CuqAy83oj8BZgFd3g3K8CX2hlAPsA/0w3RuLfgHdX1aV0IeUv6P7wfYdukPDJ85zzj9pxr6C7lPM2urE1twOvovvDeRNdT9BCbhh3fqvXd6rqyzOFVfWxduyV6WaHfQ04bAHHo6ruoQs7h7XP9G7gmKr694Xsv5V8iG5A7Tq6AeL/eb4Nq+qf6QLDR+l6Ix4PHDXH8Z7PWEBsl3peQDe4/To2BpdHtU3+jO5SznV0PWzvX2C9Z5/nHrqB58cBt7XP8o/A3W39hcCpwKV0l1Uva7ve3X4+l+77eQFd78pdrT4zTqT7Tn6b7nv5IbqQvTUcTTeO6Nt0g7ZPae0Nm2ifBbSttFXk/peTJemhKcmZdANn/3Rz2z5UJbkceG9VvW+Odb9AF1YfVlX3Trxy0kOIPT+SNFBJfiXJ/9Euex0LPAX4p7H1v5Hunks70fXY/YPBR9o8w48kDdcT6e55cxvd/aJeXFU3j61/Jd3tEr5JNz7ov066gtJDkZe9JEnSkmLPjyRJWlIMP5IkaUlZCk82XpBddtml9tprr2lXQ5IkbQWf//znv1dVs2/6Chh+Nthrr71YtWrVtKshSZK2giTzPlrGy16SJGlJMfxIkqQlxfAjSZKWFMOPJElaUgw/kiRpSTH8SJKkJcXwI0mSlhTDjyRJWlIMP5IkaUkx/EiSpCXF8CNJkpYUw0/PPnX1LXxz7Q+nXQ1JktQYfnp2/Nmf59zPr5l2NSRJUmP46VtgfdW0ayFJkhrDT89GAcw+kiQNhuGnZyH2/EiSNCCGn56NAmYfSZKGw/DTsySsN/xIkjQYhp+eJVAO+pEkaTAMPz0bJV72kiRpQAw/PYtT3SVJGhTDT8/s+ZEkaVgMPz0L9vxIkjQkhp+eJXG4syRJA2L46Vl3nx/jjyRJQ2H46VkC69dPuxaSJGmG4adno8T7/EiSNCCGn551A56nXQtJkjTD8NOzONVdkqRBMfz0LA54liRpUAw/PRs51V2SpEEx/PRs5OMtJEkaFMNPz5I44FmSpAEx/PTMMT+SJA2L4adnAWd7SZI0IIafnnmTQ0mShsXw07NR4uMtJEkakN7CT5IzktyS5GtjZX+V5N+TfCXJx5LsOLbu5CSrk1yd5JCx8kNb2eokJ42V753k8lb+kSTbtvKHteXVbf1efX3GhYizvSRJGpQ+e37OBA6dVXYx8OSqegrwDeBkgCT7AkcBT2r7vDvJsiTLgHcBhwH7Ake3bQHeBryzqp4A3Aoc18qPA25t5e9s201NvM+PJEmD0lv4qarPAOtmlX2iqu5ti5cBe7T3RwArq+ruqroOWA3s316rq+raqroHWAkckSTA84Bz2/5nAUeOHeus9v5c4KC2/VR0A56NP5IkDcU0x/z8F+DC9n534MaxdWta2XzljwZuGwtSM+X3O1Zbf3vb/qckOT7JqiSr1q5du8UfaC6jkbO9JEkakqmEnyT/DbgX+OA0zj+jqk6rqv2qar8VK1b0co4Qx/xIkjQgyyd9wiS/A7wAOKg2Xg+6CdhzbLM9WhnzlH8f2DHJ8ta7M779zLHWJFkOPKptPxWj4JgfSZIGZKI9P0kOBV4PvLCq7hxbdT5wVJuptTewD/A54Apgnzaza1u6QdHnt9B0KfDitv+xwHljxzq2vX8x8Mma4qAbH28hSdKw9Nbzk+TDwIHALknWAKfQze56GHBxG4N8WVX9XlVdmeQc4Cq6y2EnVNV97TgnAhcBy4AzqurKdoo/AVYmeQvwReD0Vn468P4kq+kGXB/V12dcCB9vIUnSsPQWfqrq6DmKT5+jbGb7twJvnaP8AuCCOcqvpZsNNrv8x8BLHlBlezRKHPAsSdKAeIfnngVvcihJ0pAYfnpmz48kScNi+OmZj7eQJGlYDD896wY8T7sWkiRphuGnZ6OE8k4/kiQNhuGnZ91lr2nXQpIkzTD89Kwb8Gz6kSRpKAw/E2DPjyRJw2H46Vk35keSJA2F4adnIx9vIUnSoBh+etY92NTwI0nSUBh+ejbyPj+SJA2K4ad3ccCzJEkDYvjpmWN+JEkaFsNPz3ywqSRJw2L46ZkPNpUkaVgMPz3zPj+SJA2L4adv9vxIkjQohp+ejRLs+pEkaTgMPz0L9vxIkjQkhp+ejez4kSRpUAw/PRv5eAtJkgbF8NO3wPr1066EJEmaYfjp2SiZdhUkSdIYw0/PHPAsSdKwGH565uMtJEkaFsNPz0Yje34kSRoSw0/vwnqzjyRJg2H46dko4J1+JEkaDsNPz7qnuk+7FpIkaYbhp2fdgGfTjyRJQ2H46Vk31X3atZAkSTMMPz2LPT+SJA2K4adn3udHkqRhMfz0rBvwbPqRJGkoDD89G8WJ7pIkDUlv4SfJGUluSfK1sbKdk1yc5Jr2c6dWniSnJlmd5CtJnjG2z7Ft+2uSHDtW/otJvtr2OTXpniA63zmmJYk9P5IkDUifPT9nAofOKjsJuKSq9gEuacsAhwH7tNfxwHugCzLAKcCzgP2BU8bCzHuAV4ztd+hmzjEVCY75kSRpQHoLP1X1GWDdrOIjgLPa+7OAI8fKz67OZcCOSR4DHAJcXFXrqupW4GLg0LbukVV1WXVTqc6eday5zjEVDniWJGlYJj3mZ9equrm9/w6wa3u/O3Dj2HZrWtmmytfMUb6pc/yUJMcnWZVk1dq1ax/Ex9m87j4/ph9JkoZiagOeW49Nr6lgc+eoqtOqar+q2m/FihW91GGUOOBZkqQBmXT4+W67ZEX7eUsrvwnYc2y7PVrZpsr3mKN8U+eYCqe6S5I0LJMOP+cDMzO2jgXOGys/ps36OgC4vV26ugg4OMlObaDzwcBFbd0dSQ5os7yOmXWsuc4xFXHMjyRJg7K8rwMn+TBwILBLkjV0s7b+AjgnyXHADcBL2+YXAIcDq4E7gZcDVNW6JG8GrmjbvamqZgZRv4puRtnDgQvbi02cYyrSflYVbTa+JEmaot7CT1UdPc+qg+bYtoAT5jnOGcAZc5SvAp48R/n35zrHtIxa4KnqLoFJkqTp8g7PPRu1wOO4H0mShsHw07NsCD/TrYckSeoYfno2M86nnPAuSdIgGH56NtPz41UvSZKGwfDTs/EBz5IkafoMPz1zwLMkScNi+OlZ2p1+DD+SJA2D4adnG8b8TLcakiSpMfz0bMNsr/VTrogkSQIMP70bbej5se9HkqQhMPz0bOaJFt7kUJKkYTD89Gw0mpnqbvqRJGkIDD89mxnzY8+PJEnDYPjp2cxlL3t+JEkaBsNPzzbc4XnK9ZAkSR3DT8/iHZ4lSRoUw0/PRj7YVJKkQTH89GzjgGfTjyRJQ2D46dnGAc9TrYYkSWoMPz3bMODZ8CNJ0iAYfnrmgGdJkobF8NMzp7pLkjQshp+e2fMjSdKwGH56Fsf8SJI0KIafnm28z4/pR5KkITD89Cz4YFNJkobE8NOzDT0/DnmWJGkQDD892zDgef106yFJkjqGn55tGPBsz48kSYNg+OmZd3iWJGlYDD89m3m2l/f5kSRpGAw/PRu1Fjb7SJI0DIafnm2c6m76kSRpCAw/PcuGqe6SJGkIDD892/h4C+OPJElDYPjp2cbHW0y3HpIkqTOV8JPkD5JcmeRrST6cZLskeye5PMnqJB9Jsm3b9mFteXVbv9fYcU5u5VcnOWSs/NBWtjrJSVP4iBvMTHX38RaSJA3DxMNPkt2B1wD7VdWTgWXAUcDbgHdW1ROAW4Hj2i7HAbe28ne27Uiyb9vvScChwLuTLEuyDHgXcBiwL3B023YqnOouSdKwTOuy13Lg4UmWAz8D3Aw8Dzi3rT8LOLK9P6It09YflG4gzRHAyqq6u6quA1YD+7fX6qq6tqruAVa2baci3uRQkqRBmXj4qaqbgLcD36ILPbcDnwduq6p722ZrgN3b+92BG9u+97btHz1ePmuf+cp/SpLjk6xKsmrt2rVb/uHmPEf30wHPkiQNwzQue+1E1xOzN7Ab8Ai6y1YTV1WnVdV+VbXfihUrejnHhsdb9HJ0SZL0QE3jstfzgeuqam1V/QT4e+A5wI7tMhjAHsBN7f1NwJ4Abf2jgO+Pl8/aZ77yqZiZ7eWYH0mShmEa4edbwAFJfqaN3TkIuAq4FHhx2+ZY4Lz2/vy2TFv/yequIZ0PHNVmg+0N7AN8DrgC2KfNHtuWblD0+RP4XHPKhvAzrRpIkqRxyze/ydZVVZcnORf4AnAv8EXgNODjwMokb2llp7ddTgfen2Q1sI4uzFBVVyY5hy443QucUFX3ASQ5EbiIbibZGVV15aQ+32ze5FCSpGGZePgBqKpTgFNmFV9LN1Nr9rY/Bl4yz3HeCrx1jvILgAu2vKZbbmaqu9lHkqRh2OxlryS7Jjk9yYVted8kx21uP3U2Dng2/UiSNAQLGfNzJt0lpN3a8jeA1/ZUn0Vnw5if9dOthyRJ6iwk/OxSVecA62HDvXbu67VWi4hT3SVJGpaFhJ8fJXk07e93kgPobjSoBYhT3SVJGpSFDHh+Hd1U8ccn+d/ACjZOSddmBGd7SZI0JJsNP1X1hSS/AjyRbvLS1e3mhFqAUetbM/tIkjQMmw0/SY6ZVfSMJFTV2T3VaVGZ6fnxJoeSJA3DQi57PXPs/XZ0d2T+AmD4WYCZx1s41V2SpGFYyGWvV48vJ9kRWNlXhRYbH28hSdKwPJhne/2I7onsWgAfbyFJ0rAsZMzPP7DxNjUjYF/gnD4rtZhsuM+P2UeSpEFYyJift4+9vxe4oarW9FSfRWfm2V7e50eSpGFYyJifT0+iIouVPT+SJA3LvOEnyQ+Y+6kMAaqqHtlbrRYR7/AsSdKwzBt+qmqHSVZkscqGqe6SJGkIFjLmB4AkP0t3nx8AqupbvdRokRk520uSpEHZ7FT3JC9Mcg1wHfBp4Hrgwp7rtWh4nx9JkoZlIff5eTNwAPCNqtqb7g7Pl/Vaq0XEAc+SJA3LQsLPT6rq+8AoyaiqLgX267lei4ZT3SVJGpaFjPm5Lcn2wGeADya5he4uz1qADXd4nnI9JElSZyE9P0cAdwJ/APwT8E3g1/us1GKyYbaXPT+SJA3CQnp+Xgl8pKpuAs7quT6LzsyYn/WOeJYkaRAW0vOzA/CJJP+S5MQku/ZdqcVk5H1+JEkalM2Gn6r6s6p6EnAC8Bjg00n+ufeaLRJpQ57t+JEkaRgW0vMz4xbgO8D3gZ/tpzqLT1oLO+ZHkqRhWMhNDl+V5FPAJcCjgVdU1VP6rthiMTPV3ewjSdIwLGTA857Aa6vqSz3XZVHacJNDR/1IkjQImw0/VXXyJCqyWG2Y7WX2kSRpEB7ImB89CBuf7WX6kSRpCAw/Pdt4k8Pp1kOSJHUWMuD5EUk3ZynJz7envG/Tf9UWh5mp7s72kiRpGBbS8/MZYLskuwOfAH4bOLPPSi0mI3t+JEkalIWEn1TVncCLgHdX1UuAJ/VbrcUjDniWJGlQFhR+kjwbeBnw8Va2rL8qLS4jBzxLkjQoCwk/rwVOBj5WVVcm+Tng0l5rtYhkw31+JEnSECzk2V6frqoXVtXb2sDn71XVa7bkpEl2THJukn9P8vUkz06yc5KLk1zTfu7Utk2SU5OsTvKVJM8YO86xbftrkhw7Vv6LSb7a9jk1MwlkShIHPEuSNBQLme31oSSPTPII4GvAVUn+eAvP+7fAP1XVfwCeCnwdOAm4pKr2oXuUxklt28OAfdrreOA9rV47A6cAzwL2B06ZCUxtm1eM7XfoFtZ3i4wSBzxLkjQQC7nstW9V3QEcCVwI7E034+tBSfIo4LnA6QBVdU9V3QYcAZzVNjurnY9WfnZ1LgN2TPIY4BDg4qpaV1W3AhcDh7Z1j6yqy6rrbjl77FhTERzzI0nSUCwk/GzT7utzJHB+Vf2ELRvCsjewFnhfki8m+bvWq7RrVd3ctvkOsGt7vztw49j+a1rZpsrXzFE+NaPEMT+SJA3EQsLP/wSuBx4BfCbJ44A7tuCcy4FnAO+pqqcDP2LjJS4AWo9N73khyfFJViVZtXbt2h7PY8+PJElDsZABz6dW1e5VdXi79HQD8KtbcM41wJqqurwtn0sXhr7bLlnRft7S1t9E92T5GXu0sk2V7zFH+Vyf7bSq2q+q9luxYsUWfKRN6wY893Z4SZL0ACxkwPOjkvz1TA9JknfQ9QI9KFX1HeDGJE9sRQcBVwHnAzMzto4FzmvvzweOabO+DgBub5fHLgIOTrJTG+h8MHBRW3dHkgPaLK9jxo41Fd2AZ9OPJElDsHwB25xBN8vrpW35t4H30d3x+cF6NfDBJNsC1wIvpwti5yQ5Drhh7HwXAIcDq4E727ZU1bokbwauaNu9qarWtfevonsEx8PpBmlfuAV13WLdgOdp1kCSJM1YSPh5fFX95tjynyX50pactKq+BOw3x6qD5ti2gBPmOc4ZdOFsdvkq4MlbUsetyanukiQNx0IGPN+V5JdnFpI8B7irvyotQg54liRpMBbS8/N7wNnt/jwAt7JxbI4WwDE/kiQNx2bDT1V9GXhqkke25TuSvBb4Ss91WzRG8dlekiQNxUIuewFd6Gl3egZ4XU/1WZSSeNlLkqSBWHD4mWWqDwp9qBl5nx9JkgbjwYYf/5Q/IHGquyRJAzHvmJ8kP2DukBO6++dogUYB86IkScMwb/ipqh0mWZHFbJSwfv20ayFJkuDBX/bSA+CDTSVJGg7DzwSMEi96SZI0EIafCbHnR5KkYTD8TMBohOOdJUkaCMPPBARvcihJ0lAYfiZgFLzPjyRJA2H4mQAHPEuSNByGn0lwqrskSYNh+JmAUXysuyRJQ2H4mYBgz48kSUNh+JmAUeJT3SVJGgjDzwT4eAtJkobD8DMBSZzqLknSQBh+JmAUcMSzJEnDYPiZgHiTQ0mSBsPwMwHdgGfTjyRJQ2D4mYBuqvu0ayFJksDwMxHdgGfTjyRJQ2D4mYBuwLMkSRoCw88E2PMjSdJwGH4mYBS8w7MkSQNh+JmAYM+PJElDYfiZgNjzI0nSYBh+JsAHm0qSNByGnwnwwaaSJA2H4WcCRolP9pIkaSAMPxNgz48kScNh+JmAOOZHkqTBmFr4SbIsyReT/GNb3jvJ5UlWJ/lIkm1b+cPa8uq2fq+xY5zcyq9OcshY+aGtbHWSkyb+4WYJ+GBTSZIGYpo9P78PfH1s+W3AO6vqCcCtwHGt/Djg1lb+zrYdSfYFjgKeBBwKvLsFqmXAu4DDgH2Bo9u2UzOKDzaVJGkophJ+kuwB/Brwd205wPOAc9smZwFHtvdHtGXa+oPa9kcAK6vq7qq6DlgN7N9eq6vq2qq6B1jZtp2absCz6UeSpCGYVs/P3wCvB9a35UcDt1XVvW15DbB7e787cCNAW397235D+ax95iv/KUmOT7Iqyaq1a9du4UeaXwLr129+O0mS1L+Jh58kLwBuqarPT/rcs1XVaVW1X1Xtt2LFit7OE6e6S5I0GMuncM7nAC9McjiwHfBI4G+BHZMsb707ewA3te1vAvYE1iRZDjwK+P5Y+YzxfeYrnwoHPEuSNBwT7/mpqpOrao+q2otuwPInq+plwKXAi9tmxwLntffnt2Xa+k9WlyTOB45qs8H2BvYBPgdcAezTZo9t285x/gQ+2rx8vIUkScMxjZ6f+fwJsDLJW4AvAqe38tOB9ydZDayjCzNU1ZVJzgGuAu4FTqiq+wCSnAhcBCwDzqiqKyf6SWYZjbzJoSRJQzHV8FNVnwI+1d5fSzdTa/Y2PwZeMs/+bwXeOkf5BcAFW7GqWyTE8CNJ0kB4h+cJSHDAsyRJA2H4mQAfbyFJ0nAYfiZgFGd7SZI0FIafCQg+3kKSpKEw/EzAKA54liRpKAw/E+CYH0mShsPwMwFxzI8kSYNh+JmAkVPdJUkaDMPPBHiTQ0mShsPwMwGjEY75kSRpIAw/E5DEqe6SJA2E4WcCggOeJUkaCsPPBIwSBzxLkjQQhp8JSHDAsyRJA2H4mYCRNzmUJGkwDD8TYs+PJEnDYPiZAHt+JEkaDsPPBIx8vIUkSYNh+JmAbsDztGshSZLA8DMR3VR3048kSUNg+JkEe34kSRoMw88EjOJj3SVJGgrDzwSMvMmhJEmDYfiZgBDDjyRJA2H4mYCRV70kSRoMw88keJNDSZIGw/AzAaN0P73RoSRJ02f4mYDQpR+nu0uSNH2GnwmY6flx0LMkSdNn+JmAUUs/Zh9JkqbP8DNB9vxIkjR9hp8JGCXTroIkSWoMPxMQx/xIkjQYhp8J2DjVfbr1kCRJhp+JmLnsZc+PJEnTZ/iZIO/zI0nS9E08/CTZM8mlSa5KcmWS32/lOye5OMk17edOrTxJTk2yOslXkjxj7FjHtu2vSXLsWPkvJvlq2+fUZLojjjcMeDb8SJI0ddPo+bkX+MOq2hc4ADghyb7AScAlVbUPcElbBjgM2Ke9jgfeA11YAk4BngXsD5wyE5jaNq8Y2+/QCXyueTngWZKk4Zh4+Kmqm6vqC+39D4CvA7sDRwBntc3OAo5s748Azq7OZcCOSR4DHAJcXFXrqupW4GLg0LbukVV1WXUP0zp77FhTMdPzY/SRJGn6pjrmJ8lewNOBy4Fdq+rmtuo7wK7t/e7AjWO7rWllmypfM0f5XOc/PsmqJKvWrl27ZR9mE+z5kSRpOKYWfpJsD3wUeG1V3TG+rvXY9J4Uquq0qtqvqvZbsWJFb+eJs70kSRqMqYSfJNvQBZ8PVtXft+LvtktWtJ+3tPKbgD3Hdt+jlW2qfI85yqdm5j4/XveSJGn6pjHbK8DpwNer6q/HVp0PzMzYOhY4b6z8mDbr6wDg9nZ57CLg4CQ7tYHOBwMXtXV3JDmgneuYsWNNRZjp+ZlmLSRJEsDyKZzzOcBvA19N8qVW9gbgL4BzkhwH3AC8tK27ADgcWA3cCbwcoKrWJXkzcEXb7k1Vta69fxVwJvBw4ML2mpoNd3i260eSpKmbePipqs8C891356A5ti/ghHmOdQZwxhzlq4Anb0E1t6qNA56nWw9JkuQdnidiZsBzOeBZkqSpM/xMwIb7/Jh9JEmaOsPPBMxc43OquyRJ02f4mYBRa2WzjyRJ02f4mYCNU91NP5IkTZvhZwJ8qLskScNh+JkAZ3tJkjQchp8JGHmfH0mSBsPwMwFOdZckaTgMPxPgVHdJkobD8DMBsedHkqTBMPxMwMZne5l+JEmaNsPPBMyM+ZEkSdNn+JmAkT0/kiQNhuFnAuJUd0mSBsPwMwHe5FCSpOEw/EzAxqnuU62GJEnC8DMRGwc8m34kSZo2w88EOOZHkqThMPxMwEzPz3rTjyRJU2f4mYCZnh+jjyRJ02f4mYC0Ic/e50eSpOkz/EzAyPHOkiQNhuFnAmbu8+OQH0mSps/wMwGjDWN+TD+SJE2b4WcC7PmRJGk4DD8TEB9sKknSYBh+JmDkXHdJkgbD8DMBG5/tZfqRJGnaDD8TMNrwVPcpV0SSJBl+JsExP5IkDYfhZwJ8sKkkScNh+JmADQOeHfEsSdLUGX4mwJ4fSZKGw/AzAQ54liRpOAw/E+BUd0mShmPRhp8khya5OsnqJCdNuS6AI34kSRqCRRl+kiwD3gUcBuwLHJ1k36lUpooRBRQ/+vFPuO++9VOpxoOxfn1x2533UPZYSZIWkeXTrkBP9gdWV9W1AElWAkcAV028Jm/5WX7uvnu4fjvgwvYC7quwnhFFuK/9XM+I9ZkpH7GebCxv21RGbTlsuKAWqA0X17Lh1DVWMrO+xtYzZ4/Uxu3uuXc966v4dkZsu/z+Obnud545zh0elAe5mxaZ8pugLeL356HkMSdewCN3fPREz7lYw8/uwI1jy2uAZ83eKMnxwPEAj33sY/upyXP/GGo913//R9x250+46557oYqq+6CK1H1Q66HWkyqo+0itB6qVzawvMrNM229G65lJizHV3s902HThp/052ZB0atZPSNWGfQEevv0ytt9mxF333McP713/U+eZ/X5TZXYe3d/iao6t+2n8s6Uts7j+61oKksn/V79Yw8+CVNVpwGkA++23Xz//xfzK6wHYq5eDS5KkB2pRjvkBbgL2HFveo5VJkqQlbrGGnyuAfZLsnWRb4Cjg/CnXSZIkDcCivOxVVfcmORG4CFgGnFFVV065WpIkaQAWZfgBqKoLgAumXQ9JkjQsi/WylyRJ0pwMP5IkaUkx/EiSpCXF8CNJkpYUw48kSVpSDD+SJGlJMfxIkqQlxfAjSZKWFMOPJElaUlLVz8PMH2qSrAVu6OnwuwDf6+nYuj/berJs78myvSfHtp6sPtr7cVW1Yq4Vhp8JSLKqqvabdj2WAtt6smzvybK9J8e2nqxJt7eXvSRJ0pJi+JEkSUuK4WcyTpt2BZYQ23qybO/Jsr0nx7aerIm2t2N+JEnSkmLPjyRJWlIMPz1KcmiSq5OsTnLStOuzGCW5PslXk3wpyapWtnOSi5Nc037uNO16PlQlOSPJLUm+NlY2Z/umc2r7vn8lyTOmV/OHnnna+o1Jbmrf7y8lOXxs3cmtra9Ocsh0av3QlWTPJJcmuSrJlUl+v5X7/d7KNtHWU/t+G356kmQZ8C7gMGBf4Ogk+063VovWr1bV08amSZ4EXFJV+wCXtGU9OGcCh84qm699DwP2aa/jgfdMqI6LxZn8dFsDvLN9v59WVRcAtH9LjgKe1PZ5d/s3Rwt3L/CHVbUvcABwQmtXv99b33xtDVP6fht++rM/sLqqrq2qe4CVwBFTrtNScQRwVnt/FnDk9Kry0FZVnwHWzSqer32PAM6uzmXAjkkeM5GKLgLztPV8jgBWVtXdVXUdsJru3xwtUFXdXFVfaO9/AHwd2B2/31vdJtp6Pr1/vw0//dkduHFseQ2b/mXrwSngE0k+n+T4VrZrVd3c3n8H2HU6VVu05mtfv/P9OLFdZjlj7BKubb0VJdkLeDpwOX6/ezWrrWFK32/Djx7qfrmqnkHXJX1CkueOr6xuOqNTGnti+/buPcDjgacBNwPvmGptFqEk2wMfBV5bVXeMr/P7vXXN0dZT+34bfvpzE7Dn2PIerUxbUVXd1H7eAnyMrmv0uzPd0e3nLdOr4aI0X/v6nd/Kquq7VXVfVa0H/j82dv3b1ltBkm3o/hh/sKr+vhX7/e7BXG09ze+34ac/VwD7JNk7ybZ0g7fOn3KdFpUkj0iyw8x74GDga3TtfGzb7FjgvOnUcNGar33PB45ps2IOAG4fu3ygB2HWmJLfoPt+Q9fWRyV5WJK96Qbhfm7S9XsoSxLgdODrVfXXY6v8fm9l87X1NL/fy7fmwbRRVd2b5ETgImAZcEZVXTnlai02uwIf6/67Yjnwoar6pyRXAOckOQ64AXjpFOv4kJbkw8CBwC5J1gCnAH/B3O17AXA43eDEO4GXT7zCD2HztPWBSZ5Gd+nleuCVAFV1ZZJzgKvoZtKcUFX3TaHaD2XPAX4b+GqSL7WyN+D3uw/ztfXR0/p+e4dnSZK0pHjZS5IkLSmGH0mStKQYfiRJ0pJi+JEkSUuK4UeSJC0phh9Jg5KkkrxjbPmPkrxxilWaV3sq9R9Nux6SHhjDj6ShuRt4UZJdpl0RSYuT4UfS0NwLnAb8wewVSfZK8sn2IMRLkjx2UwdKsizJXyW5ou3zylZ+YJLPJPl4kquTvDfJqK07OslXk3wtydvGjnVoki8k+XKSS8ZOs2+STyW5NslrtkoLSOqV4UfSEL0LeFmSR80q/3+Bs6rqKcAHgVM3c5zj6B5D8EzgmcAr2u3yoXuO0KuBfekerviiJLsBbwOeR/ewxWcmOTLJCrpnD/1mVT0VeMnYOf4DcEg73intGUaSBszHW0ganKq6I8nZwGuAu8ZWPRt4UXv/fuAvN3Oog4GnJHlxW34U3XOC7gE+V1XXwoZHS/wy8BPgU1W1tpV/EHgucB/wmaq6rtVv3dg5Pl5VdwN3J7mF7rErax74p5Y0KYYfSUP1N8AXgPdtwTECvLqqLrpfYXIg3fOExj3YZ/3cPfb+Pvx3VRo8L3tJGqTWu3IO3aWrGf8KHNXevwz4l80c5iLgv85cikry80ke0dbtn2TvNtbnt4DP0j05+leS7JJkGXA08GngMuC5M5fMkuy8xR9Q0tT4fyiShuwdwIljy68G3pfkj4G1tCdrJ/k9gKp676z9/w7YC/hCkrR9jmzrrgD+B/AE4FLgY1W1PslJbTl0l7TOa+c4Hvj7FpZuAf6vrfpJJU2MT3WXtOS0y15/VFUvmHJVJE2Bl70kSdKSYs+PJElaUuz5kSRJS4rhR5IkLSmGH0mStKQYfiRJ0pJi+JEkSUuK4UeSJC0p/z+bVQhgiQXaDAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu_OHpXKw-N6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}